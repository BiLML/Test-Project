# aura-backend/ai/inference.py
import os
import numpy as np
import cv2
import tensorflow as tf
from tensorflow.keras.applications.efficientnet import preprocess_input

# --- Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN MODEL (ÄÃ£ cáº­p nháº­t trá» vÃ o folder 'ai/') ---
MODEL_PATHS = {
    'EX': 'ai/unet_mega_fusion.keras',      
    'HE': 'ai/unet_hemorrhages.keras',      
    'SE': 'ai/unet_soft_exudates.keras',    
    'MA': 'ai/unet_microaneurysms.keras',   
    'OD': 'ai/unet_optic_disc.keras',       
    'Vessels': 'ai/unet_vessels_pro.keras', 
    'CLASSIFIER': 'ai/aura_retinal_model_final.keras' 
}

loaded_models = {}

print("â³ [AI MODULE] ÄANG KHá»I Äá»˜NG Há»† THá»NG AURA AI...")
for name, path in MODEL_PATHS.items():
    if os.path.exists(path):
        try:
            # compile=False Ä‘á»ƒ trÃ¡nh lá»—i hÃ m loss tÃ¹y chá»‰nh
            loaded_models[name] = tf.keras.models.load_model(path, compile=False)
            print(f"   âœ… ÄÃ£ táº£i Module: {name}")
        except Exception as e:
            print(f"   âŒ Lá»—i táº£i {name}: {e}")
    else:
        # Thá»­ tÃ¬m á»Ÿ thÆ° má»¥c gá»‘c náº¿u cháº¡y tá»« server
        print(f"   âš ï¸ KhÃ´ng tÃ¬m tháº¥y file táº¡i {path}. Äang thá»­ tÃ¬m Ä‘Æ°á»ng dáº«n tuyá»‡t Ä‘á»‘i...")

print(f"ğŸš€ [AI MODULE] Sáº´N SÃ€NG! ({len(loaded_models)}/{len(MODEL_PATHS)} modules)")

# --- CÃC HÃ€M Xá»¬ LÃ áº¢NH ---

def preprocess_for_segmentation(img_array, target_size=256):
    img = cv2.resize(img_array, (target_size, target_size))
    img = img / 255.0
    img = np.expand_dims(img, axis=0)
    return img

def preprocess_for_vessels_pro(img_array):
    img = cv2.resize(img_array, (512, 512))
    green_channel = img[:, :, 1]
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
    enhanced_img = clahe.apply(green_channel)
    enhanced_img = enhanced_img / 255.0
    enhanced_img = np.expand_dims(enhanced_img, axis=-1)
    enhanced_img = np.expand_dims(enhanced_img, axis=0)
    return enhanced_img

def preprocess_for_classifier(img_array):
    img = cv2.resize(img_array, (224, 224))
    img = cv2.addWeighted(img, 4, cv2.GaussianBlur(img, (0,0), 10), -4, 128)
    img = preprocess_input(img)
    img = np.expand_dims(img, axis=0)
    return img

def clean_mask(mask_array, min_size=20):
    mask_uint8 = (mask_array * 255).astype(np.uint8)
    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(mask_uint8, connectivity=8)
    cleaned_mask = np.zeros_like(mask_uint8)
    for i in range(1, num_labels):
        area = stats[i, cv2.CC_STAT_AREA]
        if area >= min_size:
            cleaned_mask[labels == i] = 255
    return cleaned_mask.astype(np.float32) / 255.0

# --- HÃ€M INFERENCE CHÃNH (ÄÆ°á»£c gá»i tá»« Main) ---
def run_aura_inference(image_bytes):
    # 1. Äá»c áº£nh
    nparr = np.frombuffer(image_bytes, np.uint8)
    original_img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
    original_rgb = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)
    
    OUT_SIZE = 256
    
    # Preprocess
    input_standard = preprocess_for_segmentation(original_rgb, target_size=OUT_SIZE)
    input_vessels = preprocess_for_vessels_pro(original_rgb)
    input_classifier = preprocess_for_classifier(original_rgb)
    
    findings = {}
    combined_mask = np.zeros((OUT_SIZE, OUT_SIZE, 3))
    
    # --- PHáº¦N 1: SEGMENTATION ---
    if 'Vessels' in loaded_models:
        pred = loaded_models['Vessels'].predict(input_vessels, verbose=0)[0]
        pred = cv2.resize(pred, (OUT_SIZE, OUT_SIZE))
        mask = (pred > 0.5).astype(np.float32)
        findings['Vessels_Density'] = np.sum(mask)
        combined_mask[:,:,1] = np.maximum(combined_mask[:,:,1], mask) 

    if 'OD' in loaded_models:
        pred = loaded_models['OD'].predict(input_standard, verbose=0)[0,:,:,0]
        mask = (pred > 0.5).astype(np.float32)
        findings['OD_Area'] = np.sum(mask)
        combined_mask[:,:,2] = np.maximum(combined_mask[:,:,2], mask)

    if 'HE' in loaded_models:
        pred = loaded_models['HE'].predict(input_standard, verbose=0)[0,:,:,0]
        mask = clean_mask((pred > 0.5).astype(np.float32), min_size=15)
        findings['HE_Count'] = np.sum(mask)
        combined_mask[:,:,0] = np.maximum(combined_mask[:,:,0], mask)

    if 'MA' in loaded_models:
        pred = loaded_models['MA'].predict(input_standard, verbose=0)[0,:,:,0]
        mask = clean_mask((pred > 0.2).astype(np.float32), min_size=5)
        findings['MA_Count'] = np.sum(mask)
        combined_mask[:,:,0] = np.maximum(combined_mask[:,:,0], mask)

    if 'EX' in loaded_models:
        pred = loaded_models['EX'].predict(input_standard, verbose=0)[0,:,:,0]
        mask = clean_mask((pred > 0.5).astype(np.float32), min_size=20)
        findings['EX_Count'] = np.sum(mask)
        combined_mask[:,:,0] = np.maximum(combined_mask[:,:,0], mask)
        combined_mask[:,:,1] = np.maximum(combined_mask[:,:,1], mask)

    if 'SE' in loaded_models:
        pred = loaded_models['SE'].predict(input_standard, verbose=0)[0,:,:,0]
        mask = clean_mask((pred > 0.3).astype(np.float32), min_size=20)
        findings['SE_Count'] = np.sum(mask)
        combined_mask[:,:,0] = np.maximum(combined_mask[:,:,0], mask)
        combined_mask[:,:,1] = np.maximum(combined_mask[:,:,1], mask)

    # --- PHáº¦N 2: CLASSIFICATION ---
    classifier_result = "KhÃ´ng xÃ¡c Ä‘á»‹nh"
    classifier_confidence = 0.0
    if 'CLASSIFIER' in loaded_models:
        preds = loaded_models['CLASSIFIER'].predict(input_classifier, verbose=0)
        class_idx = np.argmax(preds[0])
        classifier_confidence = float(np.max(preds[0]))
        CLASS_MAP = {0: "BÃ¬nh thÆ°á»ng (No DR)", 1: "Nháº¹ (Mild)", 2: "Trung bÃ¬nh (Moderate)", 3: "Náº·ng (Severe)", 4: "TÄƒng sinh (Proliferative)"}
        classifier_result = CLASS_MAP.get(class_idx, "KhÃ´ng xÃ¡c Ä‘á»‹nh")

    # --- PHáº¦N 3: LOGIC Há»˜I CHáº¨N (RULE-BASED) ---
    he_count = findings.get('HE_Count', 0)
    ma_count = findings.get('MA_Count', 0)
    se_count = findings.get('SE_Count', 0)
    ex_count = findings.get('EX_Count', 0)
    vessels_density = findings.get('Vessels_Density', 5000)
    od_area = findings.get('OD_Area', 0)

    seg_diagnosis = "BÃ¬nh thÆ°á»ng (No DR)"
    dr_score = 0

    if he_count > 800 or se_count > 200: 
        seg_diagnosis = "Náº·ng (Severe NPDR)"; dr_score = 3
    elif he_count > 80 or ex_count > 150: 
        seg_diagnosis = "Trung bÃ¬nh (Moderate NPDR)"; dr_score = 2
    elif ma_count > 20 or he_count > 20: 
        seg_diagnosis = "Nháº¹ (Mild NPDR)"; dr_score = 1
    
    final_diagnosis = seg_diagnosis
    warning_note = ""
    
    # Logic káº¿t há»£p Classifier cÅ© vÃ  Segmentation má»›i
    if "BÃ¬nh thÆ°á»ng" in classifier_result and classifier_confidence > 0.85:
        if seg_diagnosis == "Nháº¹ (Mild NPDR)":
            final_diagnosis = "BÃ¬nh thÆ°á»ng (No DR)"; dr_score = 0
            warning_note = "\nâœ… ÄÃ£ lá»c nhiá»…u: CÃ¡c vi tá»•n thÆ°Æ¡ng phÃ¡t hiá»‡n Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ lÃ  khÃ´ng Ä‘Ã¡ng ká»ƒ."
    elif "Náº·ng" in classifier_result and seg_diagnosis == "BÃ¬nh thÆ°á»ng (No DR)":
        final_diagnosis = f"Nghi ngá» {classifier_result}"
        warning_note = "\nâš ï¸ Cáº¢NH BÃO: AI tá»•ng quan tháº¥y dáº¥u hiá»‡u bá»‡nh náº·ng dÃ¹ tá»•n thÆ°Æ¡ng chÆ°a rÃµ rÃ ng."
        dr_score = 3

    # Táº¡o bÃ¡o cÃ¡o text
    risk_report = []
    if dr_score >= 1:
        risk_report.append(f"ğŸ©¸ TIá»‚U ÄÆ¯á»œNG: PhÃ¡t hiá»‡n biáº¿n chá»©ng ({final_diagnosis}).")
        if dr_score >= 3: risk_report.append("   âœ Cáº¢NH BÃO: Kiá»ƒm soÃ¡t Ä‘Æ°á»ng huyáº¿t kÃ©m. Nguy cÆ¡ biáº¿n chá»©ng tháº­n/tháº§n kinh.")
        elif dr_score == 2: risk_report.append("   âœ Bá»‡nh Ä‘ang tiáº¿n triá»ƒn. Cáº§n Ä‘iá»u chá»‰nh lá»‘i sá»‘ng.")
        else: risk_report.append("   âœ Giai Ä‘oáº¡n Ä‘áº§u. Theo dÃµi Ä‘á»‹nh ká»³.")
    else:
        risk_report.append("ğŸ©¸ TIá»‚U ÄÆ¯á»œNG: VÃµng máº¡c khá»e máº¡nh.")

    risk_report.append("\nâ¤ï¸ TIM Máº CH & HUYáº¾T ÃP:")
    if vessels_density < 2000: risk_report.append("âš ï¸ Cáº¢NH BÃO: Máº¡ch mÃ¡u thÆ°a/háº¹p. Nguy cÆ¡ Cao huyáº¿t Ã¡p.")
    elif vessels_density > 15000: risk_report.append("âš ï¸ Cáº¢NH BÃO: Máº¡ch mÃ¡u giÃ£n báº¥t thÆ°á»ng.")
    else: risk_report.append("âœ… Há»‡ thá»‘ng máº¡ch mÃ¡u á»•n Ä‘á»‹nh.")

    if od_area > 4500: risk_report.append("\nğŸ‘ï¸ GLOCOM: âš ï¸ KÃ­ch thÆ°á»›c Ä‘Ä©a thá»‹ lá»›n, nghi ngá» lÃµm gai.")

    # Táº¡o áº£nh Overlay
    img_resized = cv2.resize(original_rgb, (OUT_SIZE, OUT_SIZE)).astype(np.float32) / 255.0
    overlay = img_resized * (1 - combined_mask * 0.4) + combined_mask * 0.5
    overlay = np.clip(overlay * 255, 0, 255).astype(np.uint8)
    overlay_bgr = cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR)
    
    detailed_risk_text = "\n".join(risk_report) + warning_note
    detailed_risk_text += f"\n\n--- THÃ”NG Sá» Ká»¸ THUáº¬T ---\nâ€¢ HE: {int(he_count)} | MA: {int(ma_count)} | EX+SE: {int(ex_count+se_count)}"

    return overlay_bgr, final_diagnosis, detailed_risk_text